
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" media="all" href="style.css">
<title>Michael Laskey</title>
</head>

<body>
<div id="larger_container">
<div id="container">

<div id="logo"><h4><br></h4><a href="index.html" class="title"><h1>Mike Laskey</h1></a></div>

<div id="navcontainer">
<ul>
<li><a href="contact.html">Contact</a></li>
<li><a href="press.html">Press</a></li>
<li><a href="publications.html">Publications</a></li>
<li><a href="#" id="current" class="current_page">Research</a></li>
<li><a href="index.html">Home</a></li>
</ul>
</div>



<div id="header_contact">
</div>

<h2><a class="section">Imitation Learning</a></h2>

<div id="middle">

<p>Robotic manipulation tasks can be challenging to learn due to non-continuous dynamics, high dimensional state representations, and potentially delayed reward. Consider for example the box-closing task shown below. From an Reinforcement Learning perspective this is hard because a reward function, that provides sufficient information, could be difficult to specify.

<p style="text-align:center;">
<img src="images/box.gif" width="300" height="180" /><br />
<i>ABB YuMi Robot Closing a Box using Imitation Learning</i>
</p>


 </p>Classical robotic techniques could also be problematic for this task, because they require a model for how the box deforms under the applied forces. However, by using <i>human supervision</i> we can manually guide the robot to complete the task, via kinesthetic teaching.  The robot can then uses machine learning to infer the underlying behavior. The research field that studies this approach is known as <i>Imitation Learning</i>. </p>






</div>


<div id="just">
<h2><a class="section">Increasing Reliability</a></h2>

<div id="middle">

<p> An intuitive approach to  provide supervision to a robot is a technique known as Behavior Cloning, in which a robot observes a supervisor's policy and learns a mapping from state to control via regression. In the early 90s , this approach was applied to learn an end-to-end self driving car, known as ALVINN. However when applying this technique, the researchers observed the car would veer from the center of the road and not know how to recover. The found small mistakes in the robot's policy could compound during execution and force it to enter an area of the state space that wasnâ€™t seen in training. This difference between test and training distribution is known as <i>covariate shift</i> </p>


<p style="text-align:center;">
<img src="images/alvinn.gif" width="300" height="180" /><br />
<i>ALVINN Suffering from Covariate Shift</i>
</p>

<p> One technique to correct for covariate shift is by sampling states from the robot's policy. A common algorithm, DAgger, iteratively rolls out its current policy and asks for supervisor labels (or corrections) at the states it visits. Empirically, DAgger has been shown to reduce covariate shift and lead to robust control policies. However, when DAgger is used with human supervisors, it can be quite challenging for humans to provide retro-active feedback to the robot. We found, in a human study, DAgger actually performed worst than Behavior Cloning due to the extra cognitive burden on humans. The paper and subsequent comparisons of the two algorithms can be found <a href="https://arxiv.org/abs/1610.00850" class="body"> here </a>. </p>

 Another way to show the robot corrective examples and prevent drift is to inject noise into the supervisor's demonstrations. Our insight is that by injecting small levels of noise, we will focus on the states that the robot needs to recover from -- the states <i> at the boundary </i> of the ideal policy. This has the potential to obtain the advantages of DAgger-like methods by recovering from errors as the robot starts making them, without the disadvantage of aggregating these errors at training time, and getting the robot to dangerous states with low reward. Our paper that examines this approach in relation to DAgger and Behavior Cloning can be found <a href="papers/CORL_DART_FULL.pdf" class="body"> here </a>. </p>.


</div>

<div id="just">
<h2><a class="section">Learning in the Real World</a></h2>

<div id="middle">

<p> Another goal of my research is to apply Imitation Learning algorithms to various manipulation tasks, in order to better understand the potential benefits over classical methods. Recently, we have applied Imitation Learning to the task of <a href="http://goldberg.berkeley.edu/pubs/dex-net-2.0-Camera-Ready-RSS-2017.pdf"class="body">precision grasping</a>, and demonstrated that end-to-end learning improves over classical ICP methods for obtaining an object's pose, which is needed for grasp planning.</p>

We have also consider the sequential grasping in clutter task, where a robot must learn how to search for a goal object and reason about how to push obstacle objects out of the way. Originally, we considered the <a href="https://people.eecs.berkeley.edu/~anca/papers/CASE16_hierarichy.pdf" class="body">planar grasping in clutter </a> ,in which the viewpoint was from a fully observed top down perspective. Our system demonstrated high reliability and realtime performace can be obtained across a variety of object shapes with only a few hundred demonstrations. Shown below is our recent extension, where the robot observes the world through an eye-in-hand camera, which introduces the problem of partially observed states. 
</p>


<p style="text-align:center;">
<img src="images/hsr.gif" width="300" height="180" /><br />
<i>HSR Trained to Retrieve Object from Cupboard</i>
</p>

</p> We have finally examined how to build better  for supervisor's to provide demonstrations. One recent <a href="http://goldberg.berkeley.edu/pubs/2017-Liang-DY-Teleop_CASE_CAMERA_READY.pdf" class="body"> project </a> examined the potential benefits of using a Da Vince Surigical Robot Teleoperation unit to provide demonstrations to an ABB YuMi robot. We found that while humans at first struggle with the system they were able to eventually perform complex tasks like untying a rope. 

</p> When building teleoperation systems, we observed humans can provide inconsistent or noisey demonstrations. This can occur from either high frequency jitter motoins or them trying to over correct their failed demonstrations. Recently, we examined the types of inconistency humans have and attempted to correct for them. Our paper on this can be found <a href="http://goldberg.berkeley.edu/pubs/2017-Chuck-LfD_Cleaning_CASE_CAMERA_READY.pdf" class="body"> here.</a>  </p>



<p></p>
<p></p>

 
</div>

 




<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=7930083; 
var sc_invisible=1; 
var sc_security="0defddac"; 
</script>
<script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script>
<noscript>&lt;div class="statcounter"&gt;&lt;a title="tumblr site
counter" href="http://statcounter.com/tumblr/"
target="_blank"&gt;&lt;img class="statcounter"
src="http://c.statcounter.com/7930083/0/0defddac/1/"
alt="tumblr site counter"&gt;&lt;/a&gt;&lt;/div&gt;</noscript>
<!-- End of StatCounter Code for Default Guide -->


</body></html>